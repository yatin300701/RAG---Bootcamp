{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f4320be",
   "metadata": {},
   "source": [
    "# Building RAG system with langchain and chromaDB\n",
    "\n",
    "## Introduction\n",
    "\n",
    "RAG -\n",
    "\n",
    "    - Langchain: for developing application\n",
    "    - ChromaDB: open source vector db\n",
    "    - openAi: for embedding and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42692113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a66693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a350ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"files\",\n",
    "    glob='*.txt',\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding':'utf-8'}\n",
    ")\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e712d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks with len 6\n"
     ]
    }
   ],
   "source": [
    "# Document Splitting\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"chunks with len {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847fd1b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m## Initialise a simple Embedding model (no key needed)\u001b[39;00m\n\u001b[32m      7\u001b[39m embeddings = HuggingFaceEmbeddings(\n\u001b[32m      8\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rag learning/project-1/.venv/lib/python3.13/site-packages/langchain_huggingface/embeddings/huggingface.py:172\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_query\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m \n\u001b[32m    166\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    167\u001b[39m embed_kwargs = (\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.query_encode_kwargs\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.query_encode_kwargs) > \u001b[32m0\u001b[39m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encode_kwargs\n\u001b[32m    171\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rag learning/project-1/.venv/lib/python3.13/site-packages/langchain_huggingface/embeddings/huggingface.py:124\u001b[39m, in \u001b[36mHuggingFaceEmbeddings._embed\u001b[39m\u001b[34m(self, texts, encode_kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Embed a text using the HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    111\u001b[39m \n\u001b[32m    112\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m \n\u001b[32m    121\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[import]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m texts = [\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.multi_process:\n\u001b[32m    126\u001b[39m     pool = \u001b[38;5;28mself\u001b[39m._client.start_multi_process_pool()\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "## Initialise a simple Embedding model (no key needed)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "embeddings.embed_query(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ca609",
   "metadata": {},
   "source": [
    "### Initialize ChromaDB vector store and store the chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96ff4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persistant directory\n",
    "path=\"./chroma_db\"\n",
    "\n",
    "# init chroma db with hugging face embedding\n",
    "vector_store=Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=path,\n",
    "    collection_name=\"rag_collection\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b7e52",
   "metadata": {},
   "source": [
    "### Test similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b0bf8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'files/ai.txt'}, page_content='8. Artificial Intelligence'),\n",
       " Document(metadata={'source': 'files/ai.txt'}, page_content='Modern AI isn’t intelligent in the human sense. It’s a pattern amplifier trained on oceans of data. What makes it powerful isn’t creativity—it’s speed and scale. It can generate, compare, and'),\n",
       " Document(metadata={'source': 'files/ai.txt'}, page_content='compare, and optimize patterns faster than humans ever will, but it still lacks true understanding.'),\n",
       " Document(metadata={'source': 'files/climatchange.txt'}, page_content='Solutions exist, but they’re expensive upfront and pay off slowly, so decision-makers stall.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Artificial Intelligence?\"\n",
    "\n",
    "similar_docs=vector_store.similarity_search(query)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dee33d",
   "metadata": {},
   "source": [
    "### Similarity search \n",
    "\n",
    "Low score more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9ed5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'files/ai.txt'}, page_content='8. Artificial Intelligence'),\n",
       "  0.6806873679161072),\n",
       " (Document(metadata={'source': 'files/ai.txt'}, page_content='Modern AI isn’t intelligent in the human sense. It’s a pattern amplifier trained on oceans of data. What makes it powerful isn’t creativity—it’s speed and scale. It can generate, compare, and'),\n",
       "  0.739311695098877),\n",
       " (Document(metadata={'source': 'files/ai.txt'}, page_content='compare, and optimize patterns faster than humans ever will, but it still lacks true understanding.'),\n",
       "  1.2848765850067139),\n",
       " (Document(metadata={'source': 'files/climatchange.txt'}, page_content='Solutions exist, but they’re expensive upfront and pay off slowly, so decision-makers stall.'),\n",
       "  1.7676345109939575)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Artificial Intelligence?\"\n",
    "similar_docs = vector_store.similarity_search_with_score(query)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280c253",
   "metadata": {},
   "source": [
    "### Initialize LLM, RAG chain,prompt template , Query the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d7ec2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LLM stands for Master of Laws, which is a postgraduate academic degree typically pursued by individuals who have already completed a law degree and wish to further specialize in a specific area of law or gain expertise in a particular legal field. The LLM degree is recognized internationally and can lead to career opportunities in various legal sectors.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 12, 'total_tokens': 76, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CkZk592xWlhyFJC9v8mUUkKEH51T7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--6731ce3f-9159-4c7d-90f2-17c9453de9e7-0', usage_metadata={'input_tokens': 12, 'output_tokens': 64, 'total_tokens': 76, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "# openai_key = os.environ.get('OPENAI_KEY')\n",
    "llm=ChatOpenAI(\n",
    "    model= \"gpt-3.5-turbo\",\n",
    "    temperature= 0)\n",
    "\n",
    "respose=llm.invoke(\"What is llm?\")\n",
    "respose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478e7bf",
   "metadata": {},
   "source": [
    "### Mordern RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb68db54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1663ca3f0>, search_kwargs={})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert vector store to retriever\n",
    "retriever=vector_store.as_retriever(\n",
    "    search_kwarg={\"k\":3} ## retrive top 3 relevent chunks\n",
    ")\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9f091da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Prompt template\n",
    "\n",
    "system_prompt=\"\"\"You are an assistant for question-answering tasks.\n",
    " Use following pieces of rerieved context to ans the question. \n",
    " \n",
    " Context: {context}\"\"\"\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system_prompt),\n",
    "    (\"human\",\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55dce520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are an assistant for question-answering tasks.\\n Use following pieces of rerieved context to ans the question. \\n\\n Context: {context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1666da210>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1666da5d0>, root_client=<openai.OpenAI object at 0x1666d96d0>, root_async_client=<openai.AsyncOpenAI object at 0x1666da350>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document chain - ie combine relevent document after extracting from vector store\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f3c0a",
   "metadata": {},
   "source": [
    "This chain\n",
    "\n",
    "- takes retrieved documents\n",
    "- \"stuffs\" them into the prompts {{context}} placeholder\n",
    "- sends the complete prompt to the llm\n",
    "- returns the llm's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f898eef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1663ca3f0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are an assistant for question-answering tasks.\\n Use following pieces of rerieved context to ans the question. \\n\\n Context: {context}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1666da210>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1666da5d0>, root_client=<openai.OpenAI object at 0x1666d96d0>, root_async_client=<openai.AsyncOpenAI object at 0x1666da350>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final RAG chain\n",
    "\n",
    "rag_chain=create_retrieval_chain(retriever,document_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "593a315a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is real challange in climate change',\n",
       " 'context': [Document(metadata={'source': 'files/climatchange.txt'}, page_content=\"Climate change isn’t a debate; it's a measurement. CO₂ rises, temperatures follow, and feedback loops accelerate the problem. The real challenge isn’t science—it’s politics and economics. Solutions\"),\n",
       "  Document(metadata={'source': 'files/climatchange.txt'}, page_content=\"Climate change isn’t a debate; it's a measurement. CO₂ rises, temperatures follow, and feedback loops accelerate the problem. The real challenge isn’t science—it’s politics and economics. Solutions\"),\n",
       "  Document(metadata={'source': 'files/climatchange.txt'}, page_content='9. Climate Change'),\n",
       "  Document(metadata={'source': 'files/climatchange.txt'}, page_content='9. Climate Change')],\n",
       " 'answer': 'The real challenge in climate change is not the science behind it, but rather the politics and economics involved in finding and implementing solutions.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"input\":\"What is real challange in climate change\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
